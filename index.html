<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Libin Liu</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Welcome</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="bio.html">Brief&nbsp;Bio</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Libin Liu</h1>
</div>
<table class="imgtable"><tr><td>
<img src="Libin.jpg" alt="At ST Francis" width="220px" />&nbsp;</td>
<td align="left"><p>Libin Liu <br />
Ph.D. Candidate <a href="CV_Libin.pdf" target=&ldquo;blank&rdquo;>(CV)</a>  <br />
<a href="https://minghsiehece.usc.edu/" target=&ldquo;blank&rdquo;>Ming Hsieh Department of Electrical and Computer Engineering</a> <br />
<a href="http://usc.edu/" target=&ldquo;blank&rdquo;>University of Southern California</a> <br /><br />
Advisor: <a href="http://ee.usc.edu/~ubli/ubli.html" target=&ldquo;blank&rdquo;>Prof. Urbashi Mitra</a></p>
</td></tr></table>
<p><br />
Welcome to my personal website.
<br /><br />
I am working on the application of Graph Signal Processing (GSP) theory/Deep Learning (DL) on wireless network control problems that can be modeled as Markov Decision Processes (MDPs) or Reinforcement Learning (RL) problems.</p>
<h2>Research Highlight</h2>
<table class="imgtable"><tr><td>
<img src="work.jpg" alt="alt text" width="360px" />&nbsp;</td>
<td align="left"><p>Sequential decision making problem can be well-modeled as a MDP or RL problem, while classical dynamic programming algorithms can be applied to obtain the optimal policy, the value function or optimal policy can be viewed as signals defined on each state with connections to other states (e.g., a Finite State Machine). On the other hand, GSP also provides an efficient representation for data in many domains. The main goal of my research is to seek reduced dimension representation of the value function/optimal policy using tools from GSP to help in efficient algorithm design. <br /><br /></p>
<p><b>GSP on DL (currently working on)</b></p>
<ul>
<li><p>Efficient DL algorithm design with GSP for structured optimal policy in RL.</p>
</li>
</ul>
<p><b>GSP on RL</b></p>
<ul>
<li><p>Novel &ldquo;image&rdquo; graph representation of the optimal policy that facilitates the design policy sampling and interpolation algorithm, which achieves asymptotic zero policy error. </p>
</li>
</ul>
<p><b>GSP on MDPs</b> </p>
<ul>
<li><p>Proper subspace design using GSP for reduced dimension MDPs, perfect reconstruction of optimal policy is achieved.</p>
</li>
</ul>
</td></tr></table>
<h2>Contact</h2>
<p>libinliu [at] usc [dot] edu <br /><br />
EEB 540, 3740 McClintock Avenue <br />
Los Angeles, CA, 90007 <br /></p>
</td>
</tr>
</table>
</body>
</html>
